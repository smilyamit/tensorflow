
#1
Note: The number of batches is equal to number of iterations for one epoch.
Letâ€™s say we have 2000 training examples (rows)that we are going to use.
batch_size = 500
steps_per_epoch/iteration/no of Batches = 4
1 epoch = 500*4 = 2000

#2
-in ff(feed forward) network we calculate loss function
-in bp(back propagation) network we update the weights based on loss function

#3 tf.constant
All eager `tf.Tensor` values are immutable (in contrast to
`tf.Variable`). There is nothing especially _constant_ about the value
returned from `tf.constant`. This function it is not fundamentally different
from `tf.convert_to_tensor`.


#4
TensorFlow Linear Functions
y = xW + b   # y-output, x- input, W-weights, b-bias

Weights and Bias in TensorFlow
-The goal of training a neural network is to modify weights and biases to best predict the labels. 
In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out tf.placeholder() and
tf.constant(), 
-since those Tensors can't be modified. This is where tf.Variable class comes in.

#5
randn means randomly normally distributed no
randint means randomly uniformly distributed no

#6 SL vs UL vs Semi SL vs RL

SL:      both lable(output) and features (input data) are needed
UL:      no label, only feature will be given
Semi SL: for some data label will be given, for some dataset no label, feature will be there

RL:      we learn this optimal action not from a label but from a time-delayed label called a reward.
        This scalar value tells us whether the outcome of whatever we did was good or bad. Hence, the goal of RL is to 
        take actions in order to maximize reward.
        
#7
-Activation function is only applied to hidden layers

#8 Optimizer based on Gradient Descents Types
1 Batch gradient descent
2 Stochaistic Gradient Descent
3 Gradient Deccent
4. Minibatch Gradient Descent
5 Adam Optimizer



